Practical 5

import numpy as np
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# Load IMDb dataset
max_features = 20000
maxlen = 80
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)

# Pad sequences
x_train = pad_sequences(x_train, maxlen=maxlen, padding='post')
x_test = pad_sequences(x_test, maxlen=maxlen, padding='post')

# Define model
model = Sequential()
model.add(Embedding(max_features, 128))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train model
model.fit(x_train, y_train, batch_size=32, epochs=1, validation_data=(x_test, y_test))


# Evaluate above model on a given sentence
new_review = "This movie was okay!"
word_index = imdb.get_word_index()
new_review_indices = [word_index.get(word, 0) for word in new_review.split()]  # 0 for unknown words
new_review_encoded = tf.keras.preprocessing.sequence.pad_sequences([new_review_indices], maxlen=maxlen)
prediction = model.predict(new_review_encoded)
print(f"Sentiment: {'Positive' if prediction > 0.5 else 'Negative'}")


Practical 4 $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$




pip install -q optuna

import optuna
import tensorflow as tf
from tensorflow.keras import datasets, layers, models

# Load CIFAR-10 dataset
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# Shrinking Data
train_images=train_images[:300]
train_labels=train_labels[:300]
test_images=test_images[:100]
test_labels=test_labels[:100]

# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0

def create_model(trial):
    # Define hyperparameters to be optimized
    optimizer = trial.suggest_categorical('optimizer', ['adam', 'rmsprop', 'sgd'])
    dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)
    num_filters = trial.suggest_int('num_filters', 32, 128)

    model = models.Sequential([
        layers.Conv2D(num_filters, (3, 3), activation='relu', input_shape=(32, 32, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(num_filters * 2, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(num_filters * 2, (3, 3), activation='relu'),
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(10)
    ])

    model.compile(optimizer=optimizer,
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    return model

def objective(trial):
    # Create model and train
    model = create_model(trial)
    model.fit(train_images, train_labels, epochs=1, batch_size=32, verbose=0)

    # Evaluate model
    _, accuracy = model.evaluate(test_images, test_labels)
    return accuracy

# Set up Optuna study and optimize
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=5)

# Print results
best_trial = study.best_trial
print("Best trial:")
print("  Value: ", best_trial.value)
print("  Params: ")
for key, value in best_trial.params.items():
    print("    {}: {}".format(key, value))





import tensorflow
import keras
from keras.datasets import mnist
from matplotlib import pyplot as plt
(trainX, trainy), (testX, testy) = mnist.load_data()
print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))
print('Test: X=%s, y=%s' % (testX.shape, testy.shape))
for i in range(9):
    plt.subplot(330 + 1 + i)
    plt.imshow(trainX[i], cmap=plt.get_cmap('gray'))
plt.show()




import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix

(trainX, trainY), (testX, testY) = mnist.load_data()

trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
testX = testX.reshape((testX.shape[0], 28, 28, 1))
trainY = to_categorical(trainY)
testY = to_categorical(testY)

model = load_model('final_model.h5')

y_pred = model.predict(testX)

cm = confusion_matrix(np.argmax(testY, axis=1), np.argmax(y_pred, axis=1))
print(cm)
