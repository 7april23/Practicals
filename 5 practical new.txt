import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# Load IMDb dataset
vocab_size = 10000
max_len = 200
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

# Define RNN model
model = Sequential()
model.add(Embedding(vocab_size, 128, input_length=max_len))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train RNN model
model.fit(X_train, y_train, batch_size=32, epochs=3, validation_data=(X_test, y_test))

# Test with a single sentence
def test_sentence(sentence):
    word_index = imdb.get_word_index()
    words = sentence.split()
    seq = [word_index[word] if word in word_index and word_index[word] < vocab_size else 0 for word in words]
    padded_seq = pad_sequences([seq], maxlen=max_len)
    prediction = model.predict(padded_seq)[0][0]
    print("Sentiment Analysis Result:")
    if prediction >= 0.5:
        print("Positive")
    else:
        print("Negative")

# Test with a sentence
test_sentence("This movie was fantastic, I loved every moment of it.")





import numpy as np
import networkx as nx
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense

#Create a Network Graph

G = nx.Graph()
G.add_nodes_from(['A','B','C','D','E'])
G.add_edges_from([('A','B'),('B','C'),('C','D'),('D','E'),('A','E')])

node_features = {
    'A':"This is a positive comment",
    'B':"I don't like this product",
    'C':"I have doubt about it",
    'D':"This is a negative comment",
    'E':"I like this development"
}

node_sentiments = {
    'A':1,
    'B':0,
    'C':0,
    'D':0,
    'E':1
}

for node in G.nodes():
    G.nodes[node]['text'] = node_features[node]
    G.nodes[node]['sentiment'] = node_sentiments[node]

#Text processing
tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(list(node_features.values()))
X = tokenizer.texts_to_sequences(list(node_features.values()))

vocab_size = len(tokenizer.word_index)+1
max_length = max(len(text.split()) for text in node_features.values())
X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=max_length,padding='post')

#Create an RNN model

model = Sequential()
model.add(Embedding(vocab_size,128,input_length=max_length))
model.add(LSTM(64))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

y = np.array([node_sentiments[node] for node in G.nodes()])
model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)

node_sentiments = model.predict(X)
node_sentiments

pos = nx.spring_layout(G)
node_colors = ['green' if sentiment > 0.51 else 'red' for sentiment in node_sentiments]
nx.draw(G,pos, with_labels=True,node_color=node_colors)